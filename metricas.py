
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# function that generatesconfussion matrix and metrics of the classification models
def model_evaluation(label_test, label_pred):
   """
   The function calculates and prints the confusion matrix, normalized confusion
   matrix, and ROC AUC score for a given set of test labels and predicted labels.
   
   label_test: True labels of the test data. 
   label_pred: Predicted labels or classes generated by a model
   """
   # Confussion matrix
   print('Confusion Matrix: \n',confusion_matrix(label_test,label_pred))
   print('\n')
   print('Confusion Matrix Norm: \n',confusion_matrix(label_test,label_pred, normalize='true'))
   print('\n')
   # AUC of the model
   auc=metrics.roc_auc_score(label_test, label_pred)
   print('ROC AUC: {}'.format(auc))
   print('-'*60)
   return auc
   
